[
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "Lab 6",
    "section": "",
    "text": "#Question 1:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\nlibrary(ggthemes)\n\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\ncamels &lt;- map(remote_files, read_delim, show_col_types = FALSE) |&gt; \n  power_full_join(by = 'gauge_id')\n\n\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\nscale_color_manual(values = c(\"red\", \"orange\", \"pink\"))\n\n&lt;ggproto object: Class ScaleDiscrete, Scale, gg&gt;\n    aesthetics: colour\n    axis_order: function\n    break_info: function\n    break_positions: function\n    breaks: waiver\n    call: call\n    clone: function\n    dimension: function\n    drop: TRUE\n    expand: waiver\n    get_breaks: function\n    get_breaks_minor: function\n    get_labels: function\n    get_limits: function\n    get_transformation: function\n    guide: legend\n    is_discrete: function\n    is_empty: function\n    labels: waiver\n    limits: NULL\n    make_sec_title: function\n    make_title: function\n    map: function\n    map_df: function\n    n.breaks.cache: NULL\n    na.translate: TRUE\n    na.value: grey50\n    name: waiver\n    palette: function\n    palette.cache: NULL\n    position: left\n    range: environment\n    rescale: function\n    reset: function\n    train: function\n    train_df: function\n    transform: function\n    transform_df: function\n    super:  &lt;ggproto object: Class ScaleDiscrete, Scale, gg&gt;\n\nscale_color_brewer(palette = \"Set1\")\n\n&lt;ggproto object: Class ScaleDiscrete, Scale, gg&gt;\n    aesthetics: colour\n    axis_order: function\n    break_info: function\n    break_positions: function\n    breaks: waiver\n    call: call\n    clone: function\n    dimension: function\n    drop: TRUE\n    expand: waiver\n    get_breaks: function\n    get_breaks_minor: function\n    get_labels: function\n    get_limits: function\n    get_transformation: function\n    guide: legend\n    is_discrete: function\n    is_empty: function\n    labels: waiver\n    limits: NULL\n    make_sec_title: function\n    make_title: function\n    map: function\n    map_df: function\n    n.breaks.cache: NULL\n    na.translate: TRUE\n    na.value: NA\n    name: waiver\n    palette: function\n    palette.cache: NULL\n    position: left\n    range: environment\n    rescale: function\n    reset: function\n    train: function\n    train_df: function\n    transform: function\n    transform_df: function\n    super:  &lt;ggproto object: Class ScaleDiscrete, Scale, gg&gt;\n\nscale_color_gradient(low = \"blue\", high = \"red\")\n\n&lt;ScaleContinuous&gt;\n Range:  \n Limits:    0 --    1\n\n\n#Question 2:\n\ncamels |&gt; \n  select(aridity, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  scale_color_viridis_c() +\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\n\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  step_naomit(all_predictors(), all_outcomes())\n\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n\nlm_model &lt;- linear_reg() %&gt;%\n \n  set_engine(\"lm\") %&gt;%\n  \n  set_mode(\"regression\")\n\n\nlm_wf &lt;- workflow() %&gt;%\n \n  add_recipe(rec) %&gt;%\n  \n  add_model(lm_model) %&gt;%\n\n  fit(data = camels_train) \n\n\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  61\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nlibrary(baguette)\nlibrary(ranger)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train) \n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.587\n2 rsq     standard       0.741\n3 mae     standard       0.363\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.563  0.0247    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.771  0.0259    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     2\n\n\n#Question 3: Based on the output of the code below, I would most likely move on with the linear regression model as well as the random forest model because they seem to have lowest RMSE and highest R^2 values.\n\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nxgb_model &lt;- boost_tree(trees = 1000) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\nxgb_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(xgb_model) %&gt;%\n  fit(data = camels_train)\n\nxgb_data &lt;- augment(xgb_wf, new_data = camels_test)\n\nmetrics(xgb_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.677\n2 rsq     standard       0.664\n3 mae     standard       0.430\n\nnnet_model &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\nnnet_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(nnet_model) %&gt;%\n  fit(data = camels_train)\n\nnnet_data &lt;- augment(nnet_wf, new_data = camels_test)\n\nmetrics(nnet_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.547\n2 rsq     standard       0.771\n3 mae     standard       0.345\n\nwf &lt;- workflow_set(\n  list(rec), \n  list(lm_model, rf_model, xgb_model, nnet_model)\n) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv)\n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_bag_mlp    Prepro… rmse    0.566  0.0198    10 recipe       bag_…     1\n2 recipe_bag_mlp    Prepro… rsq     0.782  0.0209    10 recipe       bag_…     1\n3 recipe_rand_fore… Prepro… rmse    0.562  0.0254    10 recipe       rand…     2\n4 recipe_rand_fore… Prepro… rsq     0.772  0.0263    10 recipe       rand…     2\n5 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     3\n7 recipe_boost_tree Prepro… rmse    0.640  0.0290    10 recipe       boos…     4\n8 recipe_boost_tree Prepro… rsq     0.716  0.0289    10 recipe       boos…     4\n\nmodel_metrics &lt;- bind_rows(\n  metrics(lm_data, truth = logQmean, estimate = .pred) %&gt;% mutate(model = \"Linear Regression\"),\n  metrics(rf_data, truth = logQmean, estimate = .pred) %&gt;% mutate(model = \"Random Forest\"),\n  metrics(xgb_data, truth = logQmean, estimate = .pred) %&gt;% mutate(model = \"XGBoost\"),\n  metrics(nnet_data, truth = logQmean, estimate = .pred) %&gt;% mutate(model = \"Neural Network\")\n)\n\nmodel_metrics %&gt;%\n  pivot_wider(names_from = .metric, values_from = .estimate) %&gt;%\n  arrange(rmse)\n\n# A tibble: 4 × 5\n  .estimator model              rmse   rsq   mae\n  &lt;chr&gt;      &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 standard   Neural Network    0.547 0.771 0.345\n2 standard   Linear Regression 0.583 0.742 0.390\n3 standard   Random Forest     0.587 0.741 0.363\n4 standard   XGBoost           0.677 0.664 0.430\n\n\n#Question 4 Build Your Own: Based on the output of the code below I decided to use the formula that I chose for logQmean because it helps normalize the distribution of the data, which will create a more legible graph. I believe that the best model is XGBOOST because it has the highest R^2 values which works best with the logQmean formula that I chose. The XG Boost model looks really well because it really represents the data well where the graph is easy to decipher. It is very interesting to analyze the observed vs predicted streamflows from the XGBoost graph.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\nlibrary(baguette)\nlibrary(ggthemes)\nlibrary(ranger)\nlibrary(xgboost)\n\nset.seed(123) \n\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) |&gt; \n  power_full_join(by = 'gauge_id')\n\n\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean)) |&gt; \n  drop_na()\n\ncamels_split &lt;- initial_split(camels, prop = 0.75)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\nrec &lt;- recipe(logQmean ~ aridity + p_mean + soil_porosity + elev_mean + pet_mean, data = camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%  \n  step_interact(terms = ~ aridity:p_mean) |&gt;  \n  step_naomit(all_predictors(), all_outcomes())\n\nbaked_data &lt;- prep(rec, camels_train) |&gt; bake(new_data = NULL)\n\n\n# 1. Random Forest\nrf_model &lt;- rand_forest(trees = 500) %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\n# 2. XGBoost\nxgb_model &lt;- boost_tree(trees = 1000) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n# 3. Neural Network (Bagged MLP)\nnnet_model &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\nwf &lt;- workflow_set(\n  list(rec), \n  list(rf_model, xgb_model, nnet_model)\n) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv)\n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.413  0.0332    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.882  0.0199    10 recipe       rand…     1\n3 recipe_bag_mlp    Prepro… rmse    0.415  0.0407    10 recipe       bag_…     2\n4 recipe_bag_mlp    Prepro… rsq     0.878  0.0250    10 recipe       bag_…     2\n5 recipe_boost_tree Prepro… rmse    0.432  0.0415    10 recipe       boos…     3\n6 recipe_boost_tree Prepro… rsq     0.874  0.0212    10 recipe       boos…     3\n\nbest_model &lt;- xgb_model  \nfinal_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(best_model) %&gt;%\n  fit(data = camels_train)\n\ntest_results &lt;- augment(final_wf, new_data = camels_test)\n\nmetrics(test_results, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.547\n2 rsq     standard       0.838\n3 mae     standard       0.304\n\nggplot(test_results, aes(x = logQmean, y = .pred, colour = aridity)) +\n  geom_point() +\n  geom_abline(linetype = 2, color = \"red\") +  # Reference line\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  theme_linedraw() +\n  labs(title = \"XGBoost Model: Observed vs Predicted Mean Streamflow\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")"
  },
  {
    "objectID": "lab6#2.html",
    "href": "lab6#2.html",
    "title": "lab6",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\nlibrary(ggthemes)\n\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\ncamels &lt;- map(remote_files, read_delim, show_col_types = FALSE) |&gt; \n  power_full_join(by = 'gauge_id')\n\n\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\nscale_color_manual(values = c(\"red\", \"orange\", \"pink\"))\n\n&lt;ggproto object: Class ScaleDiscrete, Scale, gg&gt;\n    aesthetics: colour\n    axis_order: function\n    break_info: function\n    break_positions: function\n    breaks: waiver\n    call: call\n    clone: function\n    dimension: function\n    drop: TRUE\n    expand: waiver\n    get_breaks: function\n    get_breaks_minor: function\n    get_labels: function\n    get_limits: function\n    get_transformation: function\n    guide: legend\n    is_discrete: function\n    is_empty: function\n    labels: waiver\n    limits: NULL\n    make_sec_title: function\n    make_title: function\n    map: function\n    map_df: function\n    n.breaks.cache: NULL\n    na.translate: TRUE\n    na.value: grey50\n    name: waiver\n    palette: function\n    palette.cache: NULL\n    position: left\n    range: environment\n    rescale: function\n    reset: function\n    train: function\n    train_df: function\n    transform: function\n    transform_df: function\n    super:  &lt;ggproto object: Class ScaleDiscrete, Scale, gg&gt;\n\nscale_color_brewer(palette = \"Set1\")\n\n&lt;ggproto object: Class ScaleDiscrete, Scale, gg&gt;\n    aesthetics: colour\n    axis_order: function\n    break_info: function\n    break_positions: function\n    breaks: waiver\n    call: call\n    clone: function\n    dimension: function\n    drop: TRUE\n    expand: waiver\n    get_breaks: function\n    get_breaks_minor: function\n    get_labels: function\n    get_limits: function\n    get_transformation: function\n    guide: legend\n    is_discrete: function\n    is_empty: function\n    labels: waiver\n    limits: NULL\n    make_sec_title: function\n    make_title: function\n    map: function\n    map_df: function\n    n.breaks.cache: NULL\n    na.translate: TRUE\n    na.value: NA\n    name: waiver\n    palette: function\n    palette.cache: NULL\n    position: left\n    range: environment\n    rescale: function\n    reset: function\n    train: function\n    train_df: function\n    transform: function\n    transform_df: function\n    super:  &lt;ggproto object: Class ScaleDiscrete, Scale, gg&gt;\n\nscale_color_gradient(low = \"blue\", high = \"red\")\n\n&lt;ScaleContinuous&gt;\n Range:  \n Limits:    0 --    1\n\n\n#Question 2:\n\ncamels |&gt; \n  select(aridity, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  scale_color_viridis_c() +\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\n\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  step_naomit(all_predictors(), all_outcomes())\n\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n\nlm_model &lt;- linear_reg() %&gt;%\n \n  set_engine(\"lm\") %&gt;%\n  \n  set_mode(\"regression\")\n\n\nlm_wf &lt;- workflow() %&gt;%\n \n  add_recipe(rec) %&gt;%\n  \n  add_model(lm_model) %&gt;%\n\n  fit(data = camels_train) \n\n\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  61\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nlibrary(baguette)\nlibrary(ranger)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train) \n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.587\n2 rsq     standard       0.741\n3 mae     standard       0.363\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.563  0.0247    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.771  0.0259    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     2\n\n\n#Question 3: Based on the output of the code below, I would most likely move on with the linear regression model as well as the random forest model because they seem to have lowest RMSE and highest R^2 values.\n\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nxgb_model &lt;- boost_tree(trees = 1000) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\nxgb_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(xgb_model) %&gt;%\n  fit(data = camels_train)\n\nxgb_data &lt;- augment(xgb_wf, new_data = camels_test)\n\nmetrics(xgb_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.677\n2 rsq     standard       0.664\n3 mae     standard       0.430\n\nnnet_model &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\nnnet_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(nnet_model) %&gt;%\n  fit(data = camels_train)\n\nnnet_data &lt;- augment(nnet_wf, new_data = camels_test)\n\nmetrics(nnet_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.547\n2 rsq     standard       0.771\n3 mae     standard       0.345\n\nwf &lt;- workflow_set(\n  list(rec), \n  list(lm_model, rf_model, xgb_model, nnet_model)\n) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv)\n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_bag_mlp    Prepro… rmse    0.566  0.0198    10 recipe       bag_…     1\n2 recipe_bag_mlp    Prepro… rsq     0.782  0.0209    10 recipe       bag_…     1\n3 recipe_rand_fore… Prepro… rmse    0.562  0.0254    10 recipe       rand…     2\n4 recipe_rand_fore… Prepro… rsq     0.772  0.0263    10 recipe       rand…     2\n5 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     3\n7 recipe_boost_tree Prepro… rmse    0.640  0.0290    10 recipe       boos…     4\n8 recipe_boost_tree Prepro… rsq     0.716  0.0289    10 recipe       boos…     4\n\nmodel_metrics &lt;- bind_rows(\n  metrics(lm_data, truth = logQmean, estimate = .pred) %&gt;% mutate(model = \"Linear Regression\"),\n  metrics(rf_data, truth = logQmean, estimate = .pred) %&gt;% mutate(model = \"Random Forest\"),\n  metrics(xgb_data, truth = logQmean, estimate = .pred) %&gt;% mutate(model = \"XGBoost\"),\n  metrics(nnet_data, truth = logQmean, estimate = .pred) %&gt;% mutate(model = \"Neural Network\")\n)\n\nmodel_metrics %&gt;%\n  pivot_wider(names_from = .metric, values_from = .estimate) %&gt;%\n  arrange(rmse)\n\n# A tibble: 4 × 5\n  .estimator model              rmse   rsq   mae\n  &lt;chr&gt;      &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 standard   Neural Network    0.547 0.771 0.345\n2 standard   Linear Regression 0.583 0.742 0.390\n3 standard   Random Forest     0.587 0.741 0.363\n4 standard   XGBoost           0.677 0.664 0.430\n\n\n#Question 4 Build Your Own: Based on the output of the code below I decided to use the formula that I chose for logQmean because it helps normalize the distribution of the data, which will create a more legible graph. I believe that the best model is XGBOOST because it has the highest R^2 values which works best with the logQmean formula that I chose. The XG Boost model looks really well because it really represents the data well where the graph is easy to decipher. It is very interesting to analyze the observed vs predicted streamflows from the XGBoost graph.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\nlibrary(baguette)\nlibrary(ggthemes)\nlibrary(ranger)\nlibrary(xgboost)\n\nset.seed(123) \n\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) |&gt; \n  power_full_join(by = 'gauge_id')\n\n\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean)) |&gt; \n  drop_na()\n\ncamels_split &lt;- initial_split(camels, prop = 0.75)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\nrec &lt;- recipe(logQmean ~ aridity + p_mean + soil_porosity + elev_mean + pet_mean, data = camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%  \n  step_interact(terms = ~ aridity:p_mean) |&gt;  \n  step_naomit(all_predictors(), all_outcomes())\n\nbaked_data &lt;- prep(rec, camels_train) |&gt; bake(new_data = NULL)\n\n\n# 1. Random Forest\nrf_model &lt;- rand_forest(trees = 500) %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\n# 2. XGBoost\nxgb_model &lt;- boost_tree(trees = 1000) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n# 3. Neural Network (Bagged MLP)\nnnet_model &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\nwf &lt;- workflow_set(\n  list(rec), \n  list(rf_model, xgb_model, nnet_model)\n) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv)\n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.413  0.0332    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.882  0.0199    10 recipe       rand…     1\n3 recipe_bag_mlp    Prepro… rmse    0.415  0.0407    10 recipe       bag_…     2\n4 recipe_bag_mlp    Prepro… rsq     0.878  0.0250    10 recipe       bag_…     2\n5 recipe_boost_tree Prepro… rmse    0.432  0.0415    10 recipe       boos…     3\n6 recipe_boost_tree Prepro… rsq     0.874  0.0212    10 recipe       boos…     3\n\nbest_model &lt;- xgb_model  \nfinal_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(best_model) %&gt;%\n  fit(data = camels_train)\n\ntest_results &lt;- augment(final_wf, new_data = camels_test)\n\nmetrics(test_results, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.547\n2 rsq     standard       0.838\n3 mae     standard       0.304\n\nggplot(test_results, aes(x = logQmean, y = .pred, colour = aridity)) +\n  geom_point() +\n  geom_abline(linetype = 2, color = \"red\") +  # Reference line\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  theme_linedraw() +\n  labs(title = \"XGBoost Model: Observed vs Predicted Mean Streamflow\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")"
  }
]