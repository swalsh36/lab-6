[
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "Lab 6",
    "section": "",
    "text": "#Question 1:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\nlibrary(ggthemes)\n\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\ncamels &lt;- map(remote_files, read_delim, show_col_types = FALSE) |&gt; \n  power_full_join(by = 'gauge_id')\n\n\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\nscale_color_manual(values = c(\"red\", \"orange\", \"pink\"))\n\n&lt;ggproto object: Class ScaleDiscrete, Scale, gg&gt;\n    aesthetics: colour\n    axis_order: function\n    break_info: function\n    break_positions: function\n    breaks: waiver\n    call: call\n    clone: function\n    dimension: function\n    drop: TRUE\n    expand: waiver\n    get_breaks: function\n    get_breaks_minor: function\n    get_labels: function\n    get_limits: function\n    get_transformation: function\n    guide: legend\n    is_discrete: function\n    is_empty: function\n    labels: waiver\n    limits: NULL\n    make_sec_title: function\n    make_title: function\n    map: function\n    map_df: function\n    n.breaks.cache: NULL\n    na.translate: TRUE\n    na.value: grey50\n    name: waiver\n    palette: function\n    palette.cache: NULL\n    position: left\n    range: environment\n    rescale: function\n    reset: function\n    train: function\n    train_df: function\n    transform: function\n    transform_df: function\n    super:  &lt;ggproto object: Class ScaleDiscrete, Scale, gg&gt;\n\nscale_color_brewer(palette = \"Set1\")\n\n&lt;ggproto object: Class ScaleDiscrete, Scale, gg&gt;\n    aesthetics: colour\n    axis_order: function\n    break_info: function\n    break_positions: function\n    breaks: waiver\n    call: call\n    clone: function\n    dimension: function\n    drop: TRUE\n    expand: waiver\n    get_breaks: function\n    get_breaks_minor: function\n    get_labels: function\n    get_limits: function\n    get_transformation: function\n    guide: legend\n    is_discrete: function\n    is_empty: function\n    labels: waiver\n    limits: NULL\n    make_sec_title: function\n    make_title: function\n    map: function\n    map_df: function\n    n.breaks.cache: NULL\n    na.translate: TRUE\n    na.value: NA\n    name: waiver\n    palette: function\n    palette.cache: NULL\n    position: left\n    range: environment\n    rescale: function\n    reset: function\n    train: function\n    train_df: function\n    transform: function\n    transform_df: function\n    super:  &lt;ggproto object: Class ScaleDiscrete, Scale, gg&gt;\n\nscale_color_gradient(low = \"blue\", high = \"red\")\n\n&lt;ScaleContinuous&gt;\n Range:  \n Limits:    0 --    1\n\n\n#Question 2:\n\ncamels |&gt; \n  select(aridity, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  scale_color_viridis_c() +\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\n\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  step_naomit(all_predictors(), all_outcomes())\n\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n\nlm_model &lt;- linear_reg() %&gt;%\n \n  set_engine(\"lm\") %&gt;%\n  \n  set_mode(\"regression\")\n\n\nlm_wf &lt;- workflow() %&gt;%\n \n  add_recipe(rec) %&gt;%\n  \n  add_model(lm_model) %&gt;%\n\n  fit(data = camels_train) \n\n\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  61\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nlibrary(baguette)\nlibrary(ranger)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train) \n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.587\n2 rsq     standard       0.741\n3 mae     standard       0.363\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.563  0.0247    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.771  0.0259    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     2\n\n\n#Question 3: Based on the output of the code below, I would most likely move on with the linear regression model as well as the random forest model because they seem to have lowest RMSE and highest R^2 values.\n\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nxgb_model &lt;- boost_tree(trees = 1000) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\nxgb_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(xgb_model) %&gt;%\n  fit(data = camels_train)\n\nxgb_data &lt;- augment(xgb_wf, new_data = camels_test)\n\nmetrics(xgb_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.677\n2 rsq     standard       0.664\n3 mae     standard       0.430\n\nnnet_model &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\nnnet_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(nnet_model) %&gt;%\n  fit(data = camels_train)\n\nnnet_data &lt;- augment(nnet_wf, new_data = camels_test)\n\nmetrics(nnet_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.547\n2 rsq     standard       0.771\n3 mae     standard       0.345\n\nwf &lt;- workflow_set(\n  list(rec), \n  list(lm_model, rf_model, xgb_model, nnet_model)\n) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv)\n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_bag_mlp    Prepro… rmse    0.566  0.0198    10 recipe       bag_…     1\n2 recipe_bag_mlp    Prepro… rsq     0.782  0.0209    10 recipe       bag_…     1\n3 recipe_rand_fore… Prepro… rmse    0.562  0.0254    10 recipe       rand…     2\n4 recipe_rand_fore… Prepro… rsq     0.772  0.0263    10 recipe       rand…     2\n5 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     3\n7 recipe_boost_tree Prepro… rmse    0.640  0.0290    10 recipe       boos…     4\n8 recipe_boost_tree Prepro… rsq     0.716  0.0289    10 recipe       boos…     4\n\nmodel_metrics &lt;- bind_rows(\n  metrics(lm_data, truth = logQmean, estimate = .pred) %&gt;% mutate(model = \"Linear Regression\"),\n  metrics(rf_data, truth = logQmean, estimate = .pred) %&gt;% mutate(model = \"Random Forest\"),\n  metrics(xgb_data, truth = logQmean, estimate = .pred) %&gt;% mutate(model = \"XGBoost\"),\n  metrics(nnet_data, truth = logQmean, estimate = .pred) %&gt;% mutate(model = \"Neural Network\")\n)\n\nmodel_metrics %&gt;%\n  pivot_wider(names_from = .metric, values_from = .estimate) %&gt;%\n  arrange(rmse)\n\n# A tibble: 4 × 5\n  .estimator model              rmse   rsq   mae\n  &lt;chr&gt;      &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 standard   Neural Network    0.547 0.771 0.345\n2 standard   Linear Regression 0.583 0.742 0.390\n3 standard   Random Forest     0.587 0.741 0.363\n4 standard   XGBoost           0.677 0.664 0.430\n\n\n#Question 4 Build Your Own: Based on the output of the code below I decided to use the formula that I chose for logQmean because it helps normalize the distribution of the data, which will create a more legible graph. I believe that the best model is XGBOOST because it has the highest R^2 values which works best with the logQmean formula that I chose. The XG Boost model looks really well because it really represents the data well where the graph is easy to decipher. It is very interesting to analyze the observed vs predicted streamflows from the XGBoost graph.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\nlibrary(baguette)\nlibrary(ggthemes)\nlibrary(ranger)\nlibrary(xgboost)\n\nset.seed(123) \n\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) |&gt; \n  power_full_join(by = 'gauge_id')\n\n\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean)) |&gt; \n  drop_na()\n\ncamels_split &lt;- initial_split(camels, prop = 0.75)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\nrec &lt;- recipe(logQmean ~ aridity + p_mean + soil_porosity + elev_mean + pet_mean, data = camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%  \n  step_interact(terms = ~ aridity:p_mean) |&gt;  \n  step_naomit(all_predictors(), all_outcomes())\n\nbaked_data &lt;- prep(rec, camels_train) |&gt; bake(new_data = NULL)\n\n\n# 1. Random Forest\nrf_model &lt;- rand_forest(trees = 500) %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\n# 2. XGBoost\nxgb_model &lt;- boost_tree(trees = 1000) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n# 3. Neural Network (Bagged MLP)\nnnet_model &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\nwf &lt;- workflow_set(\n  list(rec), \n  list(rf_model, xgb_model, nnet_model)\n) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv)\n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.413  0.0332    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.882  0.0199    10 recipe       rand…     1\n3 recipe_bag_mlp    Prepro… rmse    0.415  0.0407    10 recipe       bag_…     2\n4 recipe_bag_mlp    Prepro… rsq     0.878  0.0250    10 recipe       bag_…     2\n5 recipe_boost_tree Prepro… rmse    0.432  0.0415    10 recipe       boos…     3\n6 recipe_boost_tree Prepro… rsq     0.874  0.0212    10 recipe       boos…     3\n\nbest_model &lt;- xgb_model  \nfinal_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(best_model) %&gt;%\n  fit(data = camels_train)\n\ntest_results &lt;- augment(final_wf, new_data = camels_test)\n\nmetrics(test_results, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.547\n2 rsq     standard       0.838\n3 mae     standard       0.304\n\nggplot(test_results, aes(x = logQmean, y = .pred, colour = aridity)) +\n  geom_point() +\n  geom_abline(linetype = 2, color = \"red\") +  # Reference line\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  theme_linedraw() +\n  labs(title = \"XGBoost Model: Observed vs Predicted Mean Streamflow\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")"
  },
  {
    "objectID": "lab6#2.html",
    "href": "lab6#2.html",
    "title": "lab6",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\nlibrary(ggthemes)\n\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\ncamels &lt;- map(remote_files, read_delim, show_col_types = FALSE) |&gt; \n  power_full_join(by = 'gauge_id')\n\n\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\nscale_color_manual(values = c(\"red\", \"orange\", \"pink\"))\n\n&lt;ggproto object: Class ScaleDiscrete, Scale, gg&gt;\n    aesthetics: colour\n    axis_order: function\n    break_info: function\n    break_positions: function\n    breaks: waiver\n    call: call\n    clone: function\n    dimension: function\n    drop: TRUE\n    expand: waiver\n    get_breaks: function\n    get_breaks_minor: function\n    get_labels: function\n    get_limits: function\n    get_transformation: function\n    guide: legend\n    is_discrete: function\n    is_empty: function\n    labels: waiver\n    limits: NULL\n    make_sec_title: function\n    make_title: function\n    map: function\n    map_df: function\n    n.breaks.cache: NULL\n    na.translate: TRUE\n    na.value: grey50\n    name: waiver\n    palette: function\n    palette.cache: NULL\n    position: left\n    range: environment\n    rescale: function\n    reset: function\n    train: function\n    train_df: function\n    transform: function\n    transform_df: function\n    super:  &lt;ggproto object: Class ScaleDiscrete, Scale, gg&gt;\n\nscale_color_brewer(palette = \"Set1\")\n\n&lt;ggproto object: Class ScaleDiscrete, Scale, gg&gt;\n    aesthetics: colour\n    axis_order: function\n    break_info: function\n    break_positions: function\n    breaks: waiver\n    call: call\n    clone: function\n    dimension: function\n    drop: TRUE\n    expand: waiver\n    get_breaks: function\n    get_breaks_minor: function\n    get_labels: function\n    get_limits: function\n    get_transformation: function\n    guide: legend\n    is_discrete: function\n    is_empty: function\n    labels: waiver\n    limits: NULL\n    make_sec_title: function\n    make_title: function\n    map: function\n    map_df: function\n    n.breaks.cache: NULL\n    na.translate: TRUE\n    na.value: NA\n    name: waiver\n    palette: function\n    palette.cache: NULL\n    position: left\n    range: environment\n    rescale: function\n    reset: function\n    train: function\n    train_df: function\n    transform: function\n    transform_df: function\n    super:  &lt;ggproto object: Class ScaleDiscrete, Scale, gg&gt;\n\nscale_color_gradient(low = \"blue\", high = \"red\")\n\n&lt;ScaleContinuous&gt;\n Range:  \n Limits:    0 --    1\n\n\n#Question 2:\n\ncamels |&gt; \n  select(aridity, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  scale_color_viridis_c() +\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\n\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  step_naomit(all_predictors(), all_outcomes())\n\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n\nlm_model &lt;- linear_reg() %&gt;%\n \n  set_engine(\"lm\") %&gt;%\n  \n  set_mode(\"regression\")\n\n\nlm_wf &lt;- workflow() %&gt;%\n \n  add_recipe(rec) %&gt;%\n  \n  add_model(lm_model) %&gt;%\n\n  fit(data = camels_train) \n\n\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  61\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nlibrary(baguette)\nlibrary(ranger)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train) \n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.587\n2 rsq     standard       0.741\n3 mae     standard       0.363\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.563  0.0247    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.771  0.0259    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     2\n\n\n#Question 3: Based on the output of the code below, I would most likely move on with the linear regression model as well as the random forest model because they seem to have lowest RMSE and highest R^2 values.\n\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nxgb_model &lt;- boost_tree(trees = 1000) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\nxgb_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(xgb_model) %&gt;%\n  fit(data = camels_train)\n\nxgb_data &lt;- augment(xgb_wf, new_data = camels_test)\n\nmetrics(xgb_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.677\n2 rsq     standard       0.664\n3 mae     standard       0.430\n\nnnet_model &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\nnnet_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(nnet_model) %&gt;%\n  fit(data = camels_train)\n\nnnet_data &lt;- augment(nnet_wf, new_data = camels_test)\n\nmetrics(nnet_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.547\n2 rsq     standard       0.771\n3 mae     standard       0.345\n\nwf &lt;- workflow_set(\n  list(rec), \n  list(lm_model, rf_model, xgb_model, nnet_model)\n) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv)\n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_bag_mlp    Prepro… rmse    0.566  0.0198    10 recipe       bag_…     1\n2 recipe_bag_mlp    Prepro… rsq     0.782  0.0209    10 recipe       bag_…     1\n3 recipe_rand_fore… Prepro… rmse    0.562  0.0254    10 recipe       rand…     2\n4 recipe_rand_fore… Prepro… rsq     0.772  0.0263    10 recipe       rand…     2\n5 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     3\n7 recipe_boost_tree Prepro… rmse    0.640  0.0290    10 recipe       boos…     4\n8 recipe_boost_tree Prepro… rsq     0.716  0.0289    10 recipe       boos…     4\n\nmodel_metrics &lt;- bind_rows(\n  metrics(lm_data, truth = logQmean, estimate = .pred) %&gt;% mutate(model = \"Linear Regression\"),\n  metrics(rf_data, truth = logQmean, estimate = .pred) %&gt;% mutate(model = \"Random Forest\"),\n  metrics(xgb_data, truth = logQmean, estimate = .pred) %&gt;% mutate(model = \"XGBoost\"),\n  metrics(nnet_data, truth = logQmean, estimate = .pred) %&gt;% mutate(model = \"Neural Network\")\n)\n\nmodel_metrics %&gt;%\n  pivot_wider(names_from = .metric, values_from = .estimate) %&gt;%\n  arrange(rmse)\n\n# A tibble: 4 × 5\n  .estimator model              rmse   rsq   mae\n  &lt;chr&gt;      &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 standard   Neural Network    0.547 0.771 0.345\n2 standard   Linear Regression 0.583 0.742 0.390\n3 standard   Random Forest     0.587 0.741 0.363\n4 standard   XGBoost           0.677 0.664 0.430\n\n\n#Question 4 Build Your Own: Based on the output of the code below I decided to use the formula that I chose for logQmean because it helps normalize the distribution of the data, which will create a more legible graph. I believe that the best model is XGBOOST because it has the highest R^2 values which works best with the logQmean formula that I chose. The XG Boost model looks really well because it really represents the data well where the graph is easy to decipher. It is very interesting to analyze the observed vs predicted streamflows from the XGBoost graph.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\nlibrary(baguette)\nlibrary(ggthemes)\nlibrary(ranger)\nlibrary(xgboost)\n\nset.seed(123) \n\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) |&gt; \n  power_full_join(by = 'gauge_id')\n\n\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean)) |&gt; \n  drop_na()\n\ncamels_split &lt;- initial_split(camels, prop = 0.75)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\nrec &lt;- recipe(logQmean ~ aridity + p_mean + soil_porosity + elev_mean + pet_mean, data = camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%  \n  step_interact(terms = ~ aridity:p_mean) |&gt;  \n  step_naomit(all_predictors(), all_outcomes())\n\nbaked_data &lt;- prep(rec, camels_train) |&gt; bake(new_data = NULL)\n\n\n# 1. Random Forest\nrf_model &lt;- rand_forest(trees = 500) %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\n# 2. XGBoost\nxgb_model &lt;- boost_tree(trees = 1000) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n# 3. Neural Network (Bagged MLP)\nnnet_model &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\nwf &lt;- workflow_set(\n  list(rec), \n  list(rf_model, xgb_model, nnet_model)\n) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv)\n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.413  0.0332    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.882  0.0199    10 recipe       rand…     1\n3 recipe_bag_mlp    Prepro… rmse    0.415  0.0407    10 recipe       bag_…     2\n4 recipe_bag_mlp    Prepro… rsq     0.878  0.0250    10 recipe       bag_…     2\n5 recipe_boost_tree Prepro… rmse    0.432  0.0415    10 recipe       boos…     3\n6 recipe_boost_tree Prepro… rsq     0.874  0.0212    10 recipe       boos…     3\n\nbest_model &lt;- xgb_model  \nfinal_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(best_model) %&gt;%\n  fit(data = camels_train)\n\ntest_results &lt;- augment(final_wf, new_data = camels_test)\n\nmetrics(test_results, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.547\n2 rsq     standard       0.838\n3 mae     standard       0.304\n\nggplot(test_results, aes(x = logQmean, y = .pred, colour = aridity)) +\n  geom_point() +\n  geom_abline(linetype = 2, color = \"red\") +  # Reference line\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  theme_linedraw() +\n  labs(title = \"XGBoost Model: Observed vs Predicted Mean Streamflow\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")"
  },
  {
    "objectID": "hyperparameter-tuning.html",
    "href": "hyperparameter-tuning.html",
    "title": "Hyperparameter-Tuning",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\nlibrary(ggthemes)\nlibrary(visdat)\nlibrary(skimr)\n\n\nroot &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\n# download.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n#               'camels_attributes_v2.0.pdf')\n\nattribute_files &lt;- c(\n  \"camels_clim.txt\",\n  \"camels_geol.txt\",\n  \"camels_soil.txt\",\n  \"camels_topo.txt\",\n  \"camels_vege.txt\",\n  \"camels_hydro.txt\"\n)\n\n\nattribute_urls &lt;- paste0(\"data/\", attribute_files)\n\n\nattribute_tables &lt;- map(attribute_urls, ~read_delim(.x, col_types = cols())) %&gt;% \n  powerjoin::power_full_join(by = \"gauge_id\")\n\nclean_data &lt;- attribute_tables %&gt;%\n  janitor::clean_names() %&gt;%\n  distinct() %&gt;%\n  drop_na(gauge_id, gauge_lat, gauge_lon)\n\n\nset.seed(123)\n\n\ndata_split &lt;- initial_split(clean_data, prop = 0.8)\n\ntrain_data &lt;- training(data_split)\ntest_data  &lt;- testing(data_split)\n\n\nlibrary(recipes)\n\n\nmodel_recipe &lt;- recipe(q_mean ~ ., data = train_data) %&gt;%\n  step_rm(gauge_lat, gauge_lon) %&gt;%\n  step_normalize(all_numeric(), -all_outcomes()) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes())\n\n\nlibrary(rsample)\n\nset.seed(123)\n\nfolds &lt;- vfold_cv(train_data, v = 10)\n\n\nlibrary(parsnip)\n\nlm_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\n\nrf_model &lt;- rand_forest(mtry = 5, trees = 500, min_n = 5) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\n\nxgb_model &lt;- boost_tree(trees = 500, learn_rate = 0.05, tree_depth = 6) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n#Based on the code below I believe that the random forest model is the best at performing because it had the lowest RSME and it had the highest R^2. I believe that the random forest model is the best model because they are good at handling complex relationships, interactions, and non-linear patterns in the data, which can be related to the different soil, climate, vegetation, and hydrology variables.\n\nlibrary(workflowsets)\nlibrary(tidymodels)\nlibrary(ggplot2)\n\n\nmodel_specs &lt;- list(\n  lm_model  = lm_model,\n  rf_model  = rf_model,\n  xgb_model = xgb_model\n)\n\nwfs &lt;- workflow_set(\n  preproc = list(recipe = model_recipe),\n  models  = model_specs,\n  cross = TRUE\n)\n\n\nwfs_results &lt;- workflow_map(\n  wfs,\n  \"fit_resamples\",\n  resamples = folds,\n  metrics = metric_set(rmse, rsq),\n  verbose = TRUE\n)\n\ni 1 of 3 resampling: recipe_lm_model\n\n\n→ A | warning: ! There are new levels in `geol_2nd_class`: NA.\n               ℹ Consider using step_unknown() (`?recipes::step_unknown()`) before\n                 `step_dummy()` to handle missing values.\n\n\nThere were issues with some computations   A: x1\n\n\n→ B | warning: ! There are new levels in `gauge_id`: \"12141300\", \"01195100\", \"06632400\",\n                 \"03182500\", \"05123400\", \"04196800\", \"02312200\", \"01664000\", \"06043500\",\n                 \"12073500\", \"08150800\", \"06934000\", \"01542810\", \"04122200\", \"02053800\",\n                 \"12488500\", \"06360500\", \"06889200\", …, \"11522500\", and \"08023080\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values., ! There are new levels in `geol_1st_class`: \"Basic plutonic rocks\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values., ! There are new levels in `geol_2nd_class`: NA.\n               ℹ Consider using step_unknown() (`?recipes::step_unknown()`) before\n                 `step_dummy()` to handle missing values., prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1   B: x1\n→ C | warning: A correlation computation is required, but the inputs are size zero or one and\n               the standard deviation cannot be computed. `NA` will be returned.\nThere were issues with some computations   A: x1   B: x1\n→ D | warning: ! There are new levels in `gauge_id`: \"02177000\", \"10205030\", \"12114500\",\n                 \"06910800\", \"11381500\", \"12375900\", \"11253310\", \"02349900\", \"05412500\",\n                 \"06876700\", \"06352000\", \"09480000\", \"01544500\", \"02053200\", \"02152100\",\n                 \"08176900\", \"08066300\", \"02245500\", …, \"14308990\", and \"08086290\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values., ! There are new levels in `geol_2nd_class`: NA.\n               ℹ Consider using step_unknown() (`?recipes::step_unknown()`) before\n                 `step_dummy()` to handle missing values., prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\nThere were issues with some computations   A: x1   B: x1\n→ E | warning: ! There are new levels in `gauge_id`: \"08164000\", \"09508300\", \"07295000\",\n                 \"03161000\", \"02361000\", \"11532500\", \"12040500\", \"07315700\", \"01414500\",\n                 \"07145700\", \"04197100\", \"01669520\", \"06291500\", \"06344600\", \"07315200\",\n                 \"03504000\", \"06477500\", \"08025500\", …, \"01022500\", and \"07373000\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values., ! There are new levels in `geol_2nd_class`: NA.\n               ℹ Consider using step_unknown() (`?recipes::step_unknown()`) before\n                 `step_dummy()` to handle missing values., ! There are new levels in `dom_land_cover`: \" Closed Shrublands\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values., prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\nThere were issues with some computations   A: x1   B: x1\n→ F | warning: ! There are new levels in `gauge_id`: \"03463300\", \"01123000\", \"06746095\",\n                 \"11180500\", \"02216180\", \"11480390\", \"03238500\", \"01121000\", \"01583500\",\n                 \"01552500\", \"01605500\", \"02297155\", \"05593575\", \"03384450\", \"14309500\",\n                 \"08109700\", \"10173450\", \"08175000\", …, \"12414500\", and \"11148900\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values., ! There are new levels in `geol_2nd_class`: NA.\n               ℹ Consider using step_unknown() (`?recipes::step_unknown()`) before\n                 `step_dummy()` to handle missing values., prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\nThere were issues with some computations   A: x1   B: x1\n→ G | warning: ! There are new levels in `gauge_id`: \"07184000\", \"03011800\", \"02011460\",\n                 \"12056500\", \"02298123\", \"02081500\", \"05466500\", \"02064000\", \"03164000\",\n                 \"05488200\", \"03460000\", \"01532000\", \"01658500\", \"10023000\", \"03500000\",\n                 \"07301500\", \"06447000\", \"02137727\", …, \"04057510\", and \"01557500\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values., ! There are new levels in `geol_2nd_class`: NA.\n               ℹ Consider using step_unknown() (`?recipes::step_unknown()`) before\n                 `step_dummy()` to handle missing values., prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\nThere were issues with some computations   A: x1   B: x1\n→ H | warning: ! There are new levels in `gauge_id`: \"10172800\", \"06354000\", \"06440200\",\n                 \"10343500\", \"03241500\", \"01440400\", \"02011400\", \"05507600\", \"12147500\",\n                 \"08103900\", \"09505200\", \"02108000\", \"08198500\", \"08267500\", \"02070000\",\n                 \"02299950\", \"01644000\", \"06911900\", …, \"14216500\", and \"06450500\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values., ! There are new levels in `geol_2nd_class`: NA.\n               ℹ Consider using step_unknown() (`?recipes::step_unknown()`) before\n                 `step_dummy()` to handle missing values., prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\nThere were issues with some computations   A: x1   B: x1\nThere were issues with some computations   A: x6   B: x1   C: x5   D: x1   E: x…\n→ I | warning: ! There are new levels in `gauge_id`: \"09047700\", \"01170100\", \"01594950\",\n                 \"02212600\", \"03366500\", \"04213000\", \"12115500\", \"13310700\", \"03049000\",\n                 \"02469800\", \"02297310\", \"05057000\", \"08104900\", \"02082950\", \"01485500\",\n                 \"10336645\", \"08066200\", \"03159540\", …, \"02384540\", and \"07197000\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values., ! There are new levels in `geol_2nd_class`: NA.\n               ℹ Consider using step_unknown() (`?recipes::step_unknown()`) before\n                 `step_dummy()` to handle missing values., prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\nThere were issues with some computations   A: x6   B: x1   C: x5   D: x1   E: x…\n→ J | warning: ! There are new levels in `gauge_id`: \"02092500\", \"12143600\", \"05062500\",\n                  \"07291000\", \"08324000\", \"01518862\", \"02427250\", \"02125000\", \"02315500\",\n                  \"14158790\", \"07301410\", \"12390700\", \"04059500\", \"02465493\", \"06601000\",\n                  \"10244950\", \"01054200\", \"02178400\", …, \"05585000\", and \"02371500\".\n                ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                  to handle unseen values., ! There are new levels in `geol_2nd_class`: NA.\n                ℹ Consider using step_unknown() (`?recipes::step_unknown()`) before\n                  `step_dummy()` to handle missing values., ! There are new levels in `dom_land_cover`: \" Barren or Sparsely Vegetated\".\n                ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                  to handle unseen values., prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\nThere were issues with some computations   A: x6   B: x1   C: x5   D: x1   E: x…\n→ K | warning: ! There are new levels in `gauge_id`: \"02430085\", \"05393500\", \"03291780\",\n                  \"14141500\", \"12043000\", \"05487980\", \"01439500\", \"02314500\", \"14301000\",\n                  \"10263500\", \"04256000\", \"01568000\", \"06464500\", \"08101000\", \"11299600\",\n                  \"01144000\", \"01440000\", \"07167500\", …, \"14185000\", and \"10166430\".\n                ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                  to handle unseen values., ! There are new levels in `geol_2nd_class`: NA.\n                ℹ Consider using step_unknown() (`?recipes::step_unknown()`) before\n                  `step_dummy()` to handle missing values., prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\nThere were issues with some computations   A: x6   B: x1   C: x5   D: x1   E: x…\n→ L | warning: ! There are new levels in `gauge_id`: \"14185900\", \"12411000\", \"05057200\",\n                  \"14362250\", \"03144000\", \"02350900\", \"01047000\", \"02193340\", \"03015500\",\n                  \"09312600\", \"10258500\", \"02422500\", \"09513780\", \"08190500\", \"13340600\",\n                  \"03070500\", \"13161500\", \"12147600\", …, \"09066000\", and \"05131500\".\n                ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                  to handle unseen values., ! There are new levels in `geol_2nd_class`: NA and \"Pyroclastics\".\n                ℹ Consider using step_unknown() (`?recipes::step_unknown()`) before\n                  `step_dummy()` to handle missing values.\n                ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                  to handle unseen values., prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\nThere were issues with some computations   A: x6   B: x1   C: x5   D: x1   E: x…\nThere were issues with some computations   A: x10   B: x1   C: x10   D: x1   E:…\n\n✔ 1 of 3 resampling: recipe_lm_model (6.7s)\ni 2 of 3 resampling: recipe_rf_model\n→ A | warning: ! There are new levels in `geol_2nd_class`: NA.\n               ℹ Consider using step_unknown() (`?recipes::step_unknown()`) before\n                 `step_dummy()` to handle missing values.\nThere were issues with some computations   A: x1\n→ B | error:   Error: Missing data in dependent variable.\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x10   B: x9\n→ C | warning: ! There are new levels in `gauge_id`: \"14185900\", \"12411000\", \"05057200\",\n                 \"14362250\", \"03144000\", \"02350900\", \"01047000\", \"02193340\", \"03015500\",\n                 \"09312600\", \"10258500\", \"02422500\", \"09513780\", \"08190500\", \"13340600\",\n                 \"03070500\", \"13161500\", \"12147600\", …, \"09066000\", and \"05131500\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values., ! There are new levels in `geol_2nd_class`: NA and \"Pyroclastics\".\n               ℹ Consider using step_unknown() (`?recipes::step_unknown()`) before\n                 `step_dummy()` to handle missing values.\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values.\nThere were issues with some computations   A: x10   B: x9\nThere were issues with some computations   A: x10   B: x9   C: x1\n\n✔ 2 of 3 resampling: recipe_rf_model (3.2s)\ni 3 of 3 resampling: recipe_xgb_model\n→ A | warning: ! There are new levels in `geol_2nd_class`: NA.\n               ℹ Consider using step_unknown() (`?recipes::step_unknown()`) before\n                 `step_dummy()` to handle missing values.\n→ B | error:   [13:52:57] src/data/data.cc:461: Check failed: valid: Label contains NaN, infinity or a value too large.\n               Stack trace:\n                 [bt] (0) 1   xgboost.so                          0x000000010a79a534 dmlc::LogMessageFatal::~LogMessageFatal() + 116\n                 [bt] (1) 2   xgboost.so                          0x000000010a844b21 xgboost::MetaInfo::SetInfoFromHost(xgboost::GenericParameter const&, xgboost::StringView, xgboost::Json) + 4001\n                 [bt] (2) 3   xgboost.so                          0x000000010a84616b xgboost::MetaInfo::SetInfo(xgboost::GenericParameter const&, char const*, void const*, xgboost::DataType, unsigned long) + 171\n                 [bt] (3) 4   xgboost.so                          0x000000010a97bbc1 XGDMatrixSetFloatInfo + 129\n                 [bt] (4) 5   xgboost.so                          0x000000010a7953c8 XGDMatrixSetInfo_R + 776\n                 [bt] (5) 6   libR.dylib                          0x00000001048a53c8 R_doDotCall + 5704\n                 [bt] (6) 7   libR.dylib                          0x00000001048f7370 bcEval + 110576\n                 [bt] (7) 8   l\n→ C | error:   [13:52:58] src/data/data.cc:461: Check failed: valid: Label contains NaN, infinity or a value too large.\n               Stack trace:\n                 [bt] (0) 1   xgboost.so                          0x000000010a79a534 dmlc::LogMessageFatal::~LogMessageFatal() + 116\n                 [bt] (1) 2   xgboost.so                          0x000000010a844b21 xgboost::MetaInfo::SetInfoFromHost(xgboost::GenericParameter const&, xgboost::StringView, xgboost::Json) + 4001\n                 [bt] (2) 3   xgboost.so                          0x000000010a84616b xgboost::MetaInfo::SetInfo(xgboost::GenericParameter const&, char const*, void const*, xgboost::DataType, unsigned long) + 171\n                 [bt] (3) 4   xgboost.so                          0x000000010a97bbc1 XGDMatrixSetFloatInfo + 129\n                 [bt] (4) 5   xgboost.so                          0x000000010a7953c8 XGDMatrixSetInfo_R + 776\n                 [bt] (5) 6   libR.dylib                          0x00000001048a53c8 R_doDotCall + 5704\n                 [bt] (6) 7   libR.dylib                          0x00000001048f7370 bcEval + 110576\n                 [bt] (7) 8   l\n→ D | error:   [13:52:59] src/data/data.cc:461: Check failed: valid: Label contains NaN, infinity or a value too large.\n               Stack trace:\n                 [bt] (0) 1   xgboost.so                          0x000000010a79a534 dmlc::LogMessageFatal::~LogMessageFatal() + 116\n                 [bt] (1) 2   xgboost.so                          0x000000010a844b21 xgboost::MetaInfo::SetInfoFromHost(xgboost::GenericParameter const&, xgboost::StringView, xgboost::Json) + 4001\n                 [bt] (2) 3   xgboost.so                          0x000000010a84616b xgboost::MetaInfo::SetInfo(xgboost::GenericParameter const&, char const*, void const*, xgboost::DataType, unsigned long) + 171\n                 [bt] (3) 4   xgboost.so                          0x000000010a97bbc1 XGDMatrixSetFloatInfo + 129\n                 [bt] (4) 5   xgboost.so                          0x000000010a7953c8 XGDMatrixSetInfo_R + 776\n                 [bt] (5) 6   libR.dylib                          0x00000001048a53c8 R_doDotCall + 5704\n                 [bt] (6) 7   libR.dylib                          0x00000001048f7370 bcEval + 110576\n                 [bt] (7) 8   l\nThere were issues with some computations   A: x9   B: x1   C: x4   D: x3\n→ E | warning: ! There are new levels in `gauge_id`: \"14185900\", \"12411000\", \"05057200\",\n                 \"14362250\", \"03144000\", \"02350900\", \"01047000\", \"02193340\", \"03015500\",\n                 \"09312600\", \"10258500\", \"02422500\", \"09513780\", \"08190500\", \"13340600\",\n                 \"03070500\", \"13161500\", \"12147600\", …, \"09066000\", and \"05131500\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values., ! There are new levels in `geol_2nd_class`: NA and \"Pyroclastics\".\n               ℹ Consider using step_unknown() (`?recipes::step_unknown()`) before\n                 `step_dummy()` to handle missing values.\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values.\nThere were issues with some computations   A: x9   B: x1   C: x4   D: x3\nThere were issues with some computations   A: x10   B: x1   C: x4   D: x4   E: …\nThere were issues with some computations   A: x10   B: x1   C: x4   D: x4   E: …\n\n✔ 3 of 3 resampling: recipe_xgb_model (11.6s)\n\nautoplot(wfs_results)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nlibrary(parsnip)\n\n\nrf_tunable_spec &lt;- rand_forest(\n  mtry  = tune(),    \n  trees = 500,      \n  min_n = tune()     \n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\n\nlibrary(workflows)\n\nrf_workflow &lt;- workflow() %&gt;%\n  add_recipe(model_recipe) %&gt;%\n  add_model(rf_tunable_spec)\n\n\nlibrary(tune)\ndials &lt;- extract_parameter_set_dials(rf_workflow)\ndials$object\n\n[[1]]\n\n\n# Randomly Selected Predictors (quantitative)\n\n\nRange: [1, ?]\n\n\n\n[[2]]\n\n\nMinimal Node Size (quantitative)\n\n\nRange: [2, 40]\n\n\n\ndials_final &lt;- finalize(dials, train_data)\n\nmy.grid &lt;- grid_latin_hypercube(\n  dials_final,\n  size = 25\n)\n\nWarning: `grid_latin_hypercube()` was deprecated in dials 1.3.0.\nℹ Please use `grid_space_filling()` instead.\n\nhead(my.grid)\n\n# A tibble: 6 × 2\n   mtry min_n\n  &lt;int&gt; &lt;int&gt;\n1     6    15\n2    16    21\n3    32     5\n4    40    28\n5    55    28\n6    52    11\n\n\n\nlibrary(tune)\n\nmodel_params &lt;- tune_grid(\n  rf_workflow,\n  resamples = folds,\n  grid = my.grid,\n  metrics = metric_set(rmse, rsq, mae),\n  control = control_grid(save_pred = TRUE)\n)\n\n→ A | warning: ! There are new levels in `geol_2nd_class`: NA.\n               ℹ Consider using step_unknown() (`?recipes::step_unknown()`) before\n                 `step_dummy()` to handle missing values.\n\n\n→ B | error:   Error: Missing data in dependent variable.\n\n\nThere were issues with some computations   A: x1   B: x16\n\n\nThere were issues with some computations   A: x3   B: x59\n\n\nThere were issues with some computations   A: x5   B: x107\n\n\nThere were issues with some computations   A: x7   B: x154\n\n\nThere were issues with some computations   A: x9   B: x200\n\n\n→ C | warning: ! There are new levels in `gauge_id`: \"14185900\", \"12411000\", \"05057200\",\n                 \"14362250\", \"03144000\", \"02350900\", \"01047000\", \"02193340\", \"03015500\",\n                 \"09312600\", \"10258500\", \"02422500\", \"09513780\", \"08190500\", \"13340600\",\n                 \"03070500\", \"13161500\", \"12147600\", …, \"09066000\", and \"05131500\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values., ! There are new levels in `geol_2nd_class`: NA and \"Pyroclastics\".\n               ℹ Consider using step_unknown() (`?recipes::step_unknown()`) before\n                 `step_dummy()` to handle missing values.\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values.\n\n\nThere were issues with some computations   A: x9   B: x200\nThere were issues with some computations   A: x10   B: x225   C: x3\nThere were issues with some computations   A: x10   B: x225   C: x4\nThere were issues with some computations   A: x10   B: x225   C: x6\nThere were issues with some computations   A: x10   B: x225   C: x7\nThere were issues with some computations   A: x10   B: x225   C: x9\nThere were issues with some computations   A: x10   B: x225   C: x10\nThere were issues with some computations   A: x10   B: x225   C: x12\nThere were issues with some computations   A: x10   B: x225   C: x15\nThere were issues with some computations   A: x10   B: x225   C: x19\nThere were issues with some computations   A: x10   B: x225   C: x21\nThere were issues with some computations   A: x10   B: x225   C: x23\nThere were issues with some computations   A: x10   B: x225   C: x25\n\nautoplot(model_params)\n\n\n\n\n\n\n\n\n\ncollect_metrics(model_params)\n\n# A tibble: 75 × 8\n    mtry min_n .metric .estimator  mean     n std_err .config              \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1     6    15 mae     standard   0.715     1      NA Preprocessor1_Model01\n 2     6    15 rmse    standard   1.21      1      NA Preprocessor1_Model01\n 3     6    15 rsq     standard   0.837     1      NA Preprocessor1_Model01\n 4    16    21 mae     standard   0.520     1      NA Preprocessor1_Model02\n 5    16    21 rmse    standard   0.952     1      NA Preprocessor1_Model02\n 6    16    21 rsq     standard   0.875     1      NA Preprocessor1_Model02\n 7    32     5 mae     standard   0.391     1      NA Preprocessor1_Model03\n 8    32     5 rmse    standard   0.791     1      NA Preprocessor1_Model03\n 9    32     5 rsq     standard   0.899     1      NA Preprocessor1_Model03\n10    40    28 mae     standard   0.443     1      NA Preprocessor1_Model04\n# ℹ 65 more rows\n\nhp_best &lt;- select_best(model_params, metric = \"mae\")\n\n\nwf_tune &lt;- workflow() %&gt;%\n  add_model(rf_tunable_spec) %&gt;%\n  add_recipe(model_recipe)\n\nwf_final &lt;- finalize_workflow(\n  wf_tune,\n  hp_best\n)\n\n\nfinal_fit &lt;- last_fit(\n  wf_final,  \n  data_split      \n)\n\n→ A | warning: ! There are new levels in `geol_2nd_class`: NA.\n               ℹ Consider using step_unknown() (`?recipes::step_unknown()`) before\n                 `step_dummy()` to handle missing values.\n\n\nThere were issues with some computations   A: x1\n\n\n→ B | error:   Error: Missing data in dependent variable.\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1   B: x1\n\n\nWarning: All models failed. Run `show_notes(.Last.tune.result)` for more\ninformation.\n\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)\nlibrary(viridis)\n\nLoading required package: viridisLite\n\n\n\nAttaching package: 'viridis'\n\n\nThe following object is masked from 'package:scales':\n\n    viridis_pal\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)\nlibrary(viridis)\n\n\nclean_data &lt;- clean_data %&gt;%\n  filter(!is.na(q_mean))  # Remove rows where \n\nmodel_recipe &lt;- recipe(q_mean ~ ., data = clean_data) %&gt;%\n  step_rm(gauge_lat, gauge_lon) %&gt;%  # Remove lat/lon columns (not used in modeling)\n  step_unknown(all_nominal(), -all_outcomes()) %&gt;%  # Handle missing categories in categorical variables\n  step_normalize(all_numeric(), -all_outcomes()) %&gt;%  # Normalize numeric variables\n  step_dummy(all_nominal(), -all_outcomes())         # Create dummy variables for categorical variables\n\n\n\n\n\n\n\n\nfinal_model_full &lt;- fit(wf_final, clean_data)\n\nWarning: ! There are new levels in `geol_2nd_class`: NA.\nℹ Consider using step_unknown() (`?recipes::step_unknown()`) before\n  `step_dummy()` to handle missing values.\n\n# 2. Predict\npredictions_full &lt;- augment(final_model_full, clean_data)\n\nWarning: ! There are new levels in `geol_2nd_class`: NA.\nℹ Consider using step_unknown() (`?recipes::step_unknown()`) before\n  `step_dummy()` to handle missing values.\n\n# 3. Calculate residuals\npredictions_full &lt;- predictions_full %&gt;%\n  mutate(residual_sq = (q_mean - .pred)^2)\n\n# 4. Maps\npred_map &lt;- ggplot(predictions_full, aes(x = gauge_lon, y = gauge_lat)) +\n  geom_point(aes(color = .pred)) +\n  scale_color_viridis_c(option = \"plasma\") +\n  coord_fixed() +\n  labs(\n    title = \"Predicted Q Mean Across CONUS\",\n    x = \"Longitude\",\n    y = \"Latitude\",\n    color = \"Prediction\"\n  ) +\n  theme_minimal()\n\nresid_map &lt;- ggplot(predictions_full, aes(x = gauge_lon, y = gauge_lat)) +\n  geom_point(aes(color = residual_sq)) +\n  scale_color_viridis_c(option = \"magma\") +\n  coord_fixed() +\n  labs(\n    title = \"Residuals (Squared Error) Across CONUS\",\n    x = \"Longitude\",\n    y = \"Latitude\",\n    color = \"Residual^2\"\n  ) +\n  theme_minimal()\n\n# 5. Combine with patchwork\npred_map + resid_map"
  }
]